{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import joblib\n",
    "import os\n",
    "import gc\n",
    "\n",
    "from evaluation_metric import amex_metric\n",
    "from sklearn.model_selection import ParameterGrid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Parameters:\n",
    "    path = 'archive_models/Models_DART_all_10corr_5folds_validation/'\n",
    "    n_folds = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(data, models):\n",
    "\n",
    "    model_list = []\n",
    "    for path in [Parameters.path]:\n",
    "        for fname in os.listdir(path):\n",
    "            for model_name in models:\n",
    "                if model_name in fname:\n",
    "                    model_list.append(path + fname)\n",
    "\n",
    "    pred_list = []\n",
    "    for counter, model_path in enumerate(model_list):\n",
    "        if model_path.startswith(Parameters.path):\n",
    "            print(model_path)\n",
    "            model = joblib.load(model_path)\n",
    "            pred_list.append(model.predict(data))\n",
    "    \n",
    "    return pred_list, model_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_corr = [\n",
    "    \"corr_D_39-B_26\",\n",
    "    \"corr_D_48-B_4\",\n",
    "    \"corr_P_2-D_44\",\n",
    "    \"corr_D_47-B_4\",\n",
    "    \"corr_D_47-D_39\",\n",
    "    \"corr_P_2-B_4\",\n",
    "    \"corr_D_39-B_10\",\n",
    "    \"corr_D_44-B_4\",\n",
    "    \"corr_D_39-B_2\",\n",
    "    \"corr_D_46-B_4\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation = pd.read_parquet(Parameters.path + 'validation.parquet')\n",
    "validation_labels = validation['target']\n",
    "validation.drop('target', axis = 1, inplace=True)\n",
    "\n",
    "validation.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = ['HT']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Predicting the validation set...\\n')\n",
    "pred_df_validation, model_list = predict(validation, models)\n",
    "\n",
    "model_names = [model.split('/')[-1][:10] for model in model_list]\n",
    "pred_df_validation = pd.DataFrame(pred_df_validation).T\n",
    "pred_df_validation.columns = model_names\n",
    "pred_df_validation.index = validation.index\n",
    "\n",
    "del validation\n",
    "_ = gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_columns = sorted(pred_df_validation.columns)\n",
    "pred_df_validation = pred_df_validation[sorted_columns]\n",
    "pred_df_validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fold_score_dic = {}\n",
    "for col in pred_df_validation.columns:\n",
    "    score = amex_metric(validation_labels, pred_df_validation[col])\n",
    "    fold_score_dic[col] = score\n",
    "score_df = pd.DataFrame.from_dict(fold_score_dic, orient='index', columns=['score']).sort_values('score', ascending=False)\n",
    "\n",
    "score_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "high_score_folds = {}\n",
    "for i in range(Parameters.n_folds):\n",
    "    high_score_folds[f'fold_{i}'] = score_df[score_df.index.str.contains(f'fold_{i}')].iloc[0:].index.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid = list(ParameterGrid(high_score_folds))\n",
    "len_grid = len(grid)\n",
    "print(f'Number of combinations: {len_grid}')\n",
    "\n",
    "for counter, i in enumerate(grid):\n",
    "    if counter % int(len_grid/10) == 0:\n",
    "        print(f'{counter}', end = ', ')\n",
    "    score = amex_metric(validation_labels, pred_df_validation[[i['fold_0'], i['fold_1'], i['fold_2'], i['fold_3'], i['fold_4']]].mean(axis = 1))\n",
    "    grid[counter]['score'] = score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fold_df = pd.DataFrame(grid).sort_values('score', ascending = False).reset_index(drop = True)\n",
    "fold_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_score = 0\n",
    "for j in range(5):\n",
    "    choosen_folds = fold_df.iloc[j,:-1].values.tolist()\n",
    "    print(choosen_folds)\n",
    "    for i in range(50000):\n",
    "        weights = np.random.rand(len(choosen_folds))\n",
    "        weighted_prediction = pred_df_validation[choosen_folds].multiply(weights).mean(axis = 1)\n",
    "        score = amex_metric(validation_labels, weighted_prediction)\n",
    "        if score > max_score:\n",
    "            max_score = score\n",
    "            max_weights = weights\n",
    "            max_prediction = weighted_prediction\n",
    "            max_choosen_folds = choosen_folds\n",
    "            print(f'New max score: {max_score:.5f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_choosen_folds = ['HT4']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_weights = [0.9058,0.6510,0.3286,0.8022,0.0267]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del pred_df_validation, validation_labels, fold_df, grid, max_prediction\n",
    "_ = gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_first_half = pd.read_parquet('Data/test_all_slopes_corr_pcaslope_lagv2_avediff_catLastLastNAdate_part1.parquet')\n",
    "corr_col = test_first_half.columns[test_first_half.columns.str.startswith(\"corr_\")].to_list()\n",
    "corr_to_remove = set(corr_col).difference(set(top_corr))\n",
    "test_first_half.drop(corr_to_remove, axis=1, inplace=True)\n",
    "\n",
    "print('Predicting the first half...')\n",
    "pred_list_first_half, model_list_first_half = predict(test_first_half, max_choosen_folds)\n",
    "\n",
    "model_names = [model.split('/')[-1][:10] for model in model_list_first_half]\n",
    "pred_df_first_half = pd.DataFrame(pred_list_first_half).T\n",
    "pred_df_first_half.columns = model_names\n",
    "pred_df_first_half.index = test_first_half.index\n",
    "\n",
    "del test_first_half\n",
    "_ = gc.collect()\n",
    "\n",
    "test_second_half = pd.read_parquet('Data/test_all_slopes_corr_pcaslope_lagv2_avediff_catLastLastNAdate_part2.parquet')\n",
    "test_second_half.drop(corr_to_remove, axis=1, inplace=True)\n",
    "print('\\nPredicting the second half...')\n",
    "pred_list_second_half, model_list_second_half = predict(test_second_half, max_choosen_folds)\n",
    "\n",
    "model_names = [model.split('/')[-1][:10] for model in model_list_second_half]\n",
    "pred_df_second_half = pd.DataFrame(pred_list_second_half).T\n",
    "pred_df_second_half.columns = model_names\n",
    "pred_df_second_half.index = test_second_half.index\n",
    "\n",
    "del test_second_half\n",
    "_ = gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_df_first_half.shape, pred_df_second_half.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_df = pd.concat([pred_df_first_half, pred_df_second_half], axis=0)\n",
    "pred_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_df = pred_df[max_choosen_folds]\n",
    "pred_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_df.to_csv(Parameters.path + 'p_M10_HT4_folds.csv')\n",
    "# pred_df[max_choosen_folds].multiply(max_weights).mean(axis = 1).to_csv(Parameters.path + 'p_M10_HT4.csv', header=['prediction'])\n",
    "pred_df.mean(axis = 1).to_csv(Parameters.path + 'p_M10_HT4.csv', header=['prediction'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('rapids-22.06')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "468ef23ed2970eb3eae24d512361eed443dbea3050d88b5fbf8075c8ae4b100c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
