{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from evaluation_metric import lgb_amex_metric\n",
    "import os\n",
    "import gc\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import joblib\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import ParameterGrid\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "import lightgbm as lgb\n",
    "\n",
    "\n",
    "class CFG:\n",
    "    input_dir = 'Data/'\n",
    "    seed = 42\n",
    "    n_folds = 5\n",
    "    target = 'target'\n",
    "\n",
    "def seed_everything(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "\n",
    "def save_model(fold):\n",
    "    def callback(env):\n",
    "        global max_score\n",
    "        iteration = env.iteration\n",
    "        score = env.evaluation_result_list[0][2]\n",
    "        if iteration % 200 == 0:\n",
    "            print('iteration {}, score= {:.05f}, max_score= {:.05f}'.format(iteration,score, max_score))\n",
    "        if score > max_score:\n",
    "            max_score = score\n",
    "            path = 'Models/'\n",
    "            for fname in os.listdir(path):\n",
    "                if fname.startswith(\"fold_{}_{}\".format(main_fold, fold)):\n",
    "                    os.remove(os.path.join(path, fname))\n",
    "\n",
    "            print('High Score: iteration {}, score={:.05f}'.format(iteration, score))\n",
    "            joblib.dump(env.model, 'Models/fold_{}_{}_iter_{}_score_{:.05f}.pkl'.format(main_fold, fold, iteration, score))\n",
    "\n",
    "    callback.order = 0\n",
    "    return callback\n",
    "\n",
    "cat_features = [\n",
    "    \"B_30\",\n",
    "    \"B_38\",\n",
    "    \"D_114\",\n",
    "    \"D_116\",\n",
    "    \"D_117\",\n",
    "    \"D_120\",\n",
    "    \"D_126\",\n",
    "    \"D_63\",\n",
    "    \"D_64\",\n",
    "    \"D_66\",\n",
    "    \"D_68\"\n",
    "]\n",
    "cat_features = [f\"{cf}_last\" for cf in cat_features]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_evaluate(x_train_org, y_train_org, val, parameters, main_fold, weak_fold):\n",
    "    \n",
    "    \n",
    "    kfold = StratifiedKFold(n_splits = CFG.n_folds, shuffle = True, random_state = CFG.seed)\n",
    "    for fold, (trn_ind, val_ind) in enumerate(kfold.split(val, val[CFG.target])):\n",
    "        if fold == weak_fold:\n",
    "            \n",
    "            print(' ')\n",
    "            features = [col for col in x_train_org.columns if col not in ['target']]\n",
    "            print(f'Training fold {fold} with {len(features)} features...')\n",
    "            x_train, x_val = val[features].iloc[trn_ind], val[features].iloc[val_ind]\n",
    "            y_train, y_val = val[CFG.target].iloc[trn_ind], val[CFG.target].iloc[val_ind]\n",
    "            x_train_new = pd.concat([x_train_org, x_train], axis=0)\n",
    "            y_train_new = pd.concat([y_train_org, y_train], axis=0)\n",
    "        \n",
    "            lgb_train = lgb.Dataset(x_train_new, y_train_new, categorical_feature = cat_features)\n",
    "            lgb_valid = lgb.Dataset(x_val, y_val, categorical_feature = cat_features)\n",
    "            del x_train, x_val, y_train, y_val, x_train_new, y_train_new; gc.collect()\n",
    "                \n",
    "            model = lgb.train(\n",
    "                params = parameters,\n",
    "                train_set = lgb_train,\n",
    "                num_boost_round = 1000,\n",
    "                valid_sets = [lgb_valid],\n",
    "                feval = lgb_amex_metric,\n",
    "                callbacks=[save_model(fold)],\n",
    "                init_model='Models/cp_{}_{}_model.txt'.format(main_fold, weak_fold),\n",
    "                )\n",
    "\n",
    "            path = 'Models/'\n",
    "            for fname in os.listdir(path):\n",
    "                if fname.startswith(\"fold_{}_{}\".format(main_fold, fold)):\n",
    "                    model = joblib.load('Models/' + fname)\n",
    "                    model.save_model('Models/cp_{}_{}_model.txt'.format(main_fold, fold))\n",
    "\n",
    "# path = 'Models/'\n",
    "# for fname in os.listdir(path):\n",
    "#     if fname.startswith(\"fold_3_4\"):\n",
    "#         print(fname)\n",
    "#         print('{:.02f} MB'.format(os.path.getsize('Models/' + fname)/1000000))\n",
    "#         model = joblib.load('Models/' + fname)\n",
    "        \n",
    "#         model.save_model('Models/cp_3_4_model.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "    \n",
    "    'objective': ['binary'],\n",
    "    'metric': ['amex_metric'],\n",
    "    'boosting': ['dart'],\n",
    "    'seed': [42],\n",
    "    'num_leaves': [100, 110, 120],\n",
    "    'learning_rate': [0.01],\n",
    "    'drop_rate': [0.01, 0.025, 0.05, 0.2],\n",
    "    'feature_fraction': [0.20,0.30],\n",
    "    'bagging_freq': [10],\n",
    "    'bagging_fraction': [0.50],\n",
    "    'n_jobs': [-1],\n",
    "    'lambda_l2': [2],\n",
    "    'min_data_in_leaf': [40]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 1\n",
      "--------------------------------------------------\n",
      "0 24 {'bagging_fraction': 0.5, 'bagging_freq': 10, 'boosting': 'dart', 'drop_rate': 0.01, 'feature_fraction': 0.2, 'lambda_l2': 2, 'learning_rate': 0.01, 'metric': 'amex_metric', 'min_data_in_leaf': 40, 'n_jobs': -1, 'num_leaves': 100, 'objective': 'binary', 'seed': 42}\n",
      " \n",
      "Training fold 1 with 1103 features...\n",
      "[LightGBM] [Info] Number of positive: 114075, number of negative: 326481\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.227438 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 187496\n",
      "[LightGBM] [Info] Number of data points in the train set: 440556, number of used features: 1095\n",
      "High Score: iteration 6187, score=0.78989\n",
      "iteration 6200, score= 0.78884, max_score= 0.78989\n",
      "iteration 6400, score= 0.78887, max_score= 0.78989\n",
      "iteration 6600, score= 0.78870, max_score= 0.78989\n",
      "High Score: iteration 6662, score=0.78996\n",
      "High Score: iteration 6663, score=0.78996\n",
      "High Score: iteration 6666, score=0.79027\n",
      "iteration 6800, score= 0.78946, max_score= 0.79027\n",
      "iteration 7000, score= 0.78762, max_score= 0.79027\n",
      "--------------------------------------------------\n",
      "1 24 {'bagging_fraction': 0.5, 'bagging_freq': 10, 'boosting': 'dart', 'drop_rate': 0.01, 'feature_fraction': 0.2, 'lambda_l2': 2, 'learning_rate': 0.01, 'metric': 'amex_metric', 'min_data_in_leaf': 40, 'n_jobs': -1, 'num_leaves': 110, 'objective': 'binary', 'seed': 42}\n",
      " \n",
      "Training fold 1 with 1103 features...\n",
      "[LightGBM] [Info] Number of positive: 114075, number of negative: 326481\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 1.265552 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 187496\n",
      "[LightGBM] [Info] Number of data points in the train set: 440556, number of used features: 1095\n",
      "High Score: iteration 6668, score=0.79048\n",
      "High Score: iteration 6669, score=0.79048\n",
      "High Score: iteration 6673, score=0.79059\n",
      "High Score: iteration 6674, score=0.79070\n",
      "iteration 6800, score= 0.78978, max_score= 0.79070\n",
      "iteration 7000, score= 0.78899, max_score= 0.79070\n",
      "iteration 7200, score= 0.78889, max_score= 0.79070\n",
      "iteration 7400, score= 0.78974, max_score= 0.79070\n",
      "iteration 7600, score= 0.78784, max_score= 0.79070\n",
      "--------------------------------------------------\n",
      "2 24 {'bagging_fraction': 0.5, 'bagging_freq': 10, 'boosting': 'dart', 'drop_rate': 0.01, 'feature_fraction': 0.2, 'lambda_l2': 2, 'learning_rate': 0.01, 'metric': 'amex_metric', 'min_data_in_leaf': 40, 'n_jobs': -1, 'num_leaves': 120, 'objective': 'binary', 'seed': 42}\n",
      " \n",
      "Training fold 1 with 1103 features...\n",
      "[LightGBM] [Info] Number of positive: 114075, number of negative: 326481\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.215579 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 187496\n",
      "[LightGBM] [Info] Number of data points in the train set: 440556, number of used features: 1095\n",
      "High Score: iteration 6675, score=0.79081\n",
      "iteration 6800, score= 0.78948, max_score= 0.79081\n",
      "iteration 7000, score= 0.78964, max_score= 0.79081\n",
      "iteration 7200, score= 0.78986, max_score= 0.79081\n",
      "iteration 7400, score= 0.78902, max_score= 0.79081\n",
      "iteration 7600, score= 0.78870, max_score= 0.79081\n",
      "--------------------------------------------------\n",
      "3 24 {'bagging_fraction': 0.5, 'bagging_freq': 10, 'boosting': 'dart', 'drop_rate': 0.01, 'feature_fraction': 0.3, 'lambda_l2': 2, 'learning_rate': 0.01, 'metric': 'amex_metric', 'min_data_in_leaf': 40, 'n_jobs': -1, 'num_leaves': 100, 'objective': 'binary', 'seed': 42}\n",
      " \n",
      "Training fold 1 with 1103 features...\n",
      "[LightGBM] [Info] Number of positive: 114075, number of negative: 326481\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.258070 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 187496\n",
      "[LightGBM] [Info] Number of data points in the train set: 440556, number of used features: 1095\n",
      "High Score: iteration 6677, score=0.79081\n",
      "High Score: iteration 6678, score=0.79081\n",
      "High Score: iteration 6679, score=0.79081\n",
      "High Score: iteration 6680, score=0.79091\n",
      "High Score: iteration 6688, score=0.79092\n",
      "iteration 6800, score= 0.78915, max_score= 0.79092\n"
     ]
    }
   ],
   "source": [
    "grid  = list(ParameterGrid(params))\n",
    "len_grid = len(grid)\n",
    "main_fold_list = [1,3]\n",
    "global max_score\n",
    "\n",
    "for main_fold in main_fold_list:\n",
    "    x_train_org = pd.read_pickle('Output/x_train_fold_{}.pkl'.format(main_fold))\n",
    "    x_val = pd.read_pickle('Output/x_val_fold_{}.pkl'.format(main_fold))\n",
    "    y_train_org = pd.read_pickle('Output/y_train_fold_{}.pkl'.format(main_fold))\n",
    "    y_val = pd.read_pickle('Output/y_val_fold_{}.pkl'.format(main_fold))\n",
    "    val = pd.concat([x_val, y_val], axis=1)\n",
    "\n",
    "    if main_fold == 1:\n",
    "        weak_fold_list = [1,2]\n",
    "    if main_fold == 3: \n",
    "        weak_fold_list = [2,4]\n",
    "\n",
    "    for weak_fold in weak_fold_list:\n",
    "        print(main_fold, weak_fold)\n",
    "        \n",
    "        max_score = 0.785\n",
    "        for run, parameters in enumerate(grid):\n",
    "            print('-' * 50)\n",
    "            print(run, len_grid, parameters)\n",
    "            train_and_evaluate(x_train_org,y_train_org, val, parameters, main_fold, weak_fold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('rapids-22.06')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "468ef23ed2970eb3eae24d512361eed443dbea3050d88b5fbf8075c8ae4b100c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
