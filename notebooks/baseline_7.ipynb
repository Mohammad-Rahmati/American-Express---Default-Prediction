{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "from evaluation_metric import *\n",
    "from baseline_model import *\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pickle\n",
    "import gc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((458913, 637), (458913, 1))"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data = pd.read_parquet('Data/train_data_aggV3.parquet')\n",
    "train_data.set_index('customer_ID', inplace=True)\n",
    "train_labels = pd.read_pickle('Data/train_labels.pkl').loc[train_data.index]\n",
    "\n",
    "train_data.shape, train_labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([Float32Dtype(), dtype('int16'), dtype('float32'), dtype('int8'),\n",
       "       Float64Dtype(), dtype('float64')], dtype=object)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.drop(columns=['target', 'cid', 'S_2'],axis=1, inplace=True)\n",
    "train_data.dtypes.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "Float32D_columns = train_data.columns[train_data.dtypes == 'Float32'].values\n",
    "Float64D_columns = train_data.columns[train_data.dtypes == 'Float64'].values\n",
    "int16_columns = train_data.columns[train_data.dtypes == 'int16'].values\n",
    "int8_columns = train_data.columns[train_data.dtypes == 'int8'].values\n",
    "\n",
    "train_data[Float32D_columns] = train_data[Float32D_columns].astype('float32')\n",
    "train_data[Float64D_columns] = train_data[Float64D_columns].astype('float32')\n",
    "train_data[int16_columns] = train_data[int16_columns].astype('float32')\n",
    "train_data[int8_columns] = train_data[int8_columns].astype('int32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([dtype('float32'), dtype('int32'), dtype('float64')], dtype=object)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.dtypes.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold: 0 - seed: 0\n",
      "[LightGBM] [Info] Number of positive: 95062, number of negative: 272068\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.504459 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 111695\n",
      "[LightGBM] [Info] Number of data points in the train set: 367130, number of used features: 629\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.258933 -> initscore=-1.051523\n",
      "[LightGBM] [Info] Start training from score -1.051523\n",
      "Training until validation scores don't improve for 1500 rounds\n",
      "[500]\tvalid_0's binary_logloss: 0.23137\tvalid_0's AMEX: 0.775454\n",
      "[1000]\tvalid_0's binary_logloss: 0.22334\tvalid_0's AMEX: 0.785643\n",
      "[1500]\tvalid_0's binary_logloss: 0.220837\tvalid_0's AMEX: 0.789515\n",
      "[2000]\tvalid_0's binary_logloss: 0.219684\tvalid_0's AMEX: 0.790374\n",
      "[2500]\tvalid_0's binary_logloss: 0.21911\tvalid_0's AMEX: 0.791341\n",
      "[3000]\tvalid_0's binary_logloss: 0.218776\tvalid_0's AMEX: 0.791821\n",
      "[3500]\tvalid_0's binary_logloss: 0.218527\tvalid_0's AMEX: 0.791893\n",
      "[4000]\tvalid_0's binary_logloss: 0.218389\tvalid_0's AMEX: 0.792049\n",
      "[4500]\tvalid_0's binary_logloss: 0.218242\tvalid_0's AMEX: 0.792399\n",
      "[5000]\tvalid_0's binary_logloss: 0.218126\tvalid_0's AMEX: 0.792316\n",
      "Early stopping, best iteration is:\n",
      "[3716]\tvalid_0's binary_logloss: 0.218464\tvalid_0's AMEX: 0.792546\n",
      "Fold: 0 - seed: 0 - score 79.25%\n",
      "Fold: 1 - seed: 0\n",
      "[LightGBM] [Info] Number of positive: 95062, number of negative: 272068\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.637386 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 111747\n",
      "[LightGBM] [Info] Number of data points in the train set: 367130, number of used features: 629\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.258933 -> initscore=-1.051523\n",
      "[LightGBM] [Info] Start training from score -1.051523\n",
      "Training until validation scores don't improve for 1500 rounds\n",
      "[500]\tvalid_0's binary_logloss: 0.230833\tvalid_0's AMEX: 0.774184\n",
      "[1000]\tvalid_0's binary_logloss: 0.222807\tvalid_0's AMEX: 0.785566\n",
      "[1500]\tvalid_0's binary_logloss: 0.220269\tvalid_0's AMEX: 0.789475\n",
      "[2000]\tvalid_0's binary_logloss: 0.219171\tvalid_0's AMEX: 0.790309\n",
      "[2500]\tvalid_0's binary_logloss: 0.218612\tvalid_0's AMEX: 0.790825\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/home/mora/Desktop/Github/Amex_Competition/baseline_7.ipynb Cell 6'\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/mora/Desktop/Github/Amex_Competition/baseline_7.ipynb#ch0000002?line=0'>1</a>\u001b[0m models, importances, df_results, score_cv \u001b[39m=\u001b[39m base_model_lgbm(train_data, train_labels)\n",
      "File \u001b[0;32m~/Desktop/Github/Amex_Competition/baseline_model.py:53\u001b[0m, in \u001b[0;36mbase_model_lgbm\u001b[0;34m(train_data, train_labels, parameters)\u001b[0m\n\u001b[1;32m     47\u001b[0m parameter_variable \u001b[39m=\u001b[39m {\n\u001b[1;32m     48\u001b[0m     \u001b[39m'\u001b[39m\u001b[39mseed\u001b[39m\u001b[39m'\u001b[39m:seed,\n\u001b[1;32m     49\u001b[0m }\n\u001b[1;32m     51\u001b[0m parameters\u001b[39m.\u001b[39mupdate(parameter_variable)\n\u001b[0;32m---> 53\u001b[0m clf \u001b[39m=\u001b[39m lgb\u001b[39m.\u001b[39;49mtrain(parameters,\n\u001b[1;32m     54\u001b[0m                     lgb_train_data,\n\u001b[1;32m     55\u001b[0m                     valid_sets \u001b[39m=\u001b[39;49m [lgb_val_data],\n\u001b[1;32m     56\u001b[0m                     verbose_eval \u001b[39m=\u001b[39;49m \u001b[39m500\u001b[39;49m,\n\u001b[1;32m     57\u001b[0m                     feval\u001b[39m=\u001b[39;49mamex_metric_mod_lgbm,\n\u001b[1;32m     58\u001b[0m                     early_stopping_rounds\u001b[39m=\u001b[39;49m\u001b[39m1500\u001b[39;49m)\n\u001b[1;32m     60\u001b[0m score \u001b[39m=\u001b[39m amex_metric(y_va\u001b[39m.\u001b[39mreset_index(drop\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m), pd\u001b[39m.\u001b[39mSeries(clf\u001b[39m.\u001b[39mpredict(X_va))\u001b[39m.\u001b[39mrename(\u001b[39m'\u001b[39m\u001b[39mprediction\u001b[39m\u001b[39m'\u001b[39m))\n\u001b[1;32m     61\u001b[0m models[key] \u001b[39m=\u001b[39m clf\n",
      "File \u001b[0;32m~/miniconda3/envs/rapids-22.06/lib/python3.9/site-packages/lightgbm/engine.py:292\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(params, train_set, num_boost_round, valid_sets, valid_names, fobj, feval, init_model, feature_name, categorical_feature, early_stopping_rounds, evals_result, verbose_eval, learning_rates, keep_training_booster, callbacks)\u001b[0m\n\u001b[1;32m    284\u001b[0m \u001b[39mfor\u001b[39;00m cb \u001b[39min\u001b[39;00m callbacks_before_iter:\n\u001b[1;32m    285\u001b[0m     cb(callback\u001b[39m.\u001b[39mCallbackEnv(model\u001b[39m=\u001b[39mbooster,\n\u001b[1;32m    286\u001b[0m                             params\u001b[39m=\u001b[39mparams,\n\u001b[1;32m    287\u001b[0m                             iteration\u001b[39m=\u001b[39mi,\n\u001b[1;32m    288\u001b[0m                             begin_iteration\u001b[39m=\u001b[39minit_iteration,\n\u001b[1;32m    289\u001b[0m                             end_iteration\u001b[39m=\u001b[39minit_iteration \u001b[39m+\u001b[39m num_boost_round,\n\u001b[1;32m    290\u001b[0m                             evaluation_result_list\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m))\n\u001b[0;32m--> 292\u001b[0m booster\u001b[39m.\u001b[39;49mupdate(fobj\u001b[39m=\u001b[39;49mfobj)\n\u001b[1;32m    294\u001b[0m evaluation_result_list \u001b[39m=\u001b[39m []\n\u001b[1;32m    295\u001b[0m \u001b[39m# check evaluation result.\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/rapids-22.06/lib/python3.9/site-packages/lightgbm/basic.py:3021\u001b[0m, in \u001b[0;36mBooster.update\u001b[0;34m(self, train_set, fobj)\u001b[0m\n\u001b[1;32m   3019\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m__set_objective_to_none:\n\u001b[1;32m   3020\u001b[0m     \u001b[39mraise\u001b[39;00m LightGBMError(\u001b[39m'\u001b[39m\u001b[39mCannot update due to null objective function.\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m-> 3021\u001b[0m _safe_call(_LIB\u001b[39m.\u001b[39;49mLGBM_BoosterUpdateOneIter(\n\u001b[1;32m   3022\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mhandle,\n\u001b[1;32m   3023\u001b[0m     ctypes\u001b[39m.\u001b[39;49mbyref(is_finished)))\n\u001b[1;32m   3024\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m__is_predicted_cur_iter \u001b[39m=\u001b[39m [\u001b[39mFalse\u001b[39;00m \u001b[39mfor\u001b[39;00m _ \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m__num_dataset)]\n\u001b[1;32m   3025\u001b[0m \u001b[39mreturn\u001b[39;00m is_finished\u001b[39m.\u001b[39mvalue \u001b[39m==\u001b[39m \u001b[39m1\u001b[39m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "models, importances, df_results, score_cv = base_model_lgbm(train_data, train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(models, open(\"Models/models_baseline_7.pkl\", \"wb\"))\n",
    "pickle.dump(importances, open(\"Models/importances_baseline_7.pkl\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_importance(ii, features, PLOT_TOP_N = 50, figsize=(10, 10)):\n",
    "    importance_df = pd.DataFrame(data=importances, columns=features)\n",
    "    sorted_indices = importance_df.median(axis=0).sort_values(ascending=False).index\n",
    "    sorted_importance_df = importance_df.loc[:, sorted_indices]\n",
    "    plot_cols = sorted_importance_df.columns[:PLOT_TOP_N]\n",
    "    _, ax = plt.subplots(figsize=figsize)\n",
    "    ax.grid()\n",
    "    ax.set_xscale('log')\n",
    "    ax.set_ylabel('Feature')\n",
    "    ax.set_xlabel('Importance')\n",
    "    sns.boxplot(data=sorted_importance_df[plot_cols],\n",
    "                orient='h',\n",
    "                ax=ax)\n",
    "    plt.show()\n",
    "\n",
    "    \n",
    "plot_importance(np.array(importances),train_data.columns, PLOT_TOP_N = 100, figsize=(10, 20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = pd.read_pickle('Models/models_baseline_7.pkl')\n",
    "prediction_list = []\n",
    "for keys in models.keys():\n",
    "    prediction_list.append(models[keys].predict(train_data))\n",
    "\n",
    "prediction_df = pd.DataFrame(prediction_list).T\n",
    "prediction_df.index = train_data.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_train = prediction_df.mean(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.calibration import calibration_curve\n",
    "fraction_of_positives, mean_predicted_value = calibration_curve(train_labels, prediction_train, n_bins=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, figsize=(12, 6))\n",
    "plt.plot(mean_predicted_value, fraction_of_positives, 's-')\n",
    "plt.plot([0, 1], [0, 1], '--', color='gray')\n",
    "\n",
    "sns.despine(left=True, bottom=True)\n",
    "plt.gca().xaxis.set_ticks_position('none')\n",
    "plt.gca().yaxis.set_ticks_position('none')\n",
    "plt.title(\"Calibration Curve\", fontsize=20); pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.isotonic import IsotonicRegression\n",
    "calibr = IsotonicRegression()\n",
    "calibr.fit(prediction_train,train_labels.target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_cali_train = calibr.predict(prediction_train)\n",
    "fraction_of_positives, mean_predicted_value = calibration_curve(train_labels, prediction_cali_train, n_bins=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, figsize=(12, 6))\n",
    "plt.plot(mean_predicted_value, fraction_of_positives, 's-')\n",
    "plt.plot([0, 1], [0, 1], '--', color='gray')\n",
    "\n",
    "sns.despine(left=True, bottom=True)\n",
    "plt.gca().xaxis.set_ticks_position('none')\n",
    "plt.gca().yaxis.set_ticks_position('none')\n",
    "plt.title(\"Calibration Curve\", fontsize=20); pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = pd.read_pickle('Data/test_agg_mo.pkl')\n",
    "test_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_list = []\n",
    "for keys in models.keys():\n",
    "    prediction_list.append(models[keys].predict(test_data))\n",
    "\n",
    "prediction_df = pd.DataFrame(prediction_list).T\n",
    "prediction_df.index = test_data.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_test = prediction_df.mean(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_test_cali = calibr.predict(prediction_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_test_cali_df = pd.DataFrame(prediction_test_cali, index=test_data.index, columns=['prediction'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "indx = prediction_test_cali_df[prediction_test_cali_df['prediction'].isnull()].index.to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_df.mean(axis=1).loc[indx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_test_cali_df.loc[indx]['prediction'] = prediction_df.mean(axis=1).loc[indx].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_test_cali_df.fillna(0.999, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_test_cali_df.to_csv('Output/b7_calibrated.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_df.mean(axis=1).to_csv('Output/b7.csv', header=['prediction'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_df.loc['639c24b93e9cd49257a59e5b31abf955f2339d536771983c6acddc50050f1945']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('rapids-22.06')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "468ef23ed2970eb3eae24d512361eed443dbea3050d88b5fbf8075c8ae4b100c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
