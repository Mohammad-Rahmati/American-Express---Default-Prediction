{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "\n",
    "from baseline_model import base_model_lgbm\n",
    "\n",
    "import gc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = pd.read_pickle('Data/train_data.pkl')\n",
    "test_data = pd.read_pickle('Data/test_data.pkl')\n",
    "train_labels = pd.read_pickle('Data/train_labels.pkl')\n",
    "train = pd.read_pickle('Data/train.pkl')\n",
    "categorical_features = ['B_30', 'B_31', 'B_38', 'D_114', 'D_116', 'D_117', 'D_120', 'D_126', 'D_63', 'D_64', 'D_66', 'D_68']\n",
    "train_data.shape, test_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data[categorical_features] = train_data[categorical_features].astype(str)\n",
    "test_data[categorical_features] = test_data[categorical_features].astype(str)\n",
    "numerical_features = train_data.columns[train_data.dtypes == 'float16']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for feature in categorical_features:\n",
    "    print(feature, train_data[feature].unique())\n",
    "\n",
    "for feature in categorical_features:\n",
    "    print(feature, test_data[feature].unique())\n",
    "\n",
    "for feature in categorical_features:\n",
    "    print(feature, set(train_data[feature].unique()).difference(set(test_data[feature].unique())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numeric_transformer = Pipeline(steps=[\n",
    "    ('Imputer', SimpleImputer(strategy='median')),\n",
    "    ('Scaler', StandardScaler())])\n",
    "\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    ('Imputer', SimpleImputer(strategy='constant', fill_value='missing')),\n",
    "    ('OneHotEncoder', OneHotEncoder(drop='first'))])\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('Numerical', numeric_transformer, numerical_features),\n",
    "        ('Categorical', categorical_transformer, categorical_features)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_processed = pd.DataFrame(preprocessor.fit_transform(train_data), index=train_data.index)\n",
    "train_processed.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = train_processed.groupby('customer_ID').tail(1)\n",
    "train.to_pickle('Data/train.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del train_data, train_processed\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_labels = pd.read_pickle('Data/train_labels.pkl')\n",
    "train = pd.read_pickle('Data/train.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold: 0 - seed: 0\n",
      "[LightGBM] [Info] Number of positive: 59414, number of negative: 170042\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.060184 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 40080\n",
      "[LightGBM] [Info] Number of data points in the train set: 229456, number of used features: 210\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.258934 -> initscore=-1.051516\n",
      "[LightGBM] [Info] Start training from score -1.051516\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "[100]\ttraining's binary_logloss: 0.408692\ttraining's AMEX: 0.74684\tvalid_1's binary_logloss: 0.410069\tvalid_1's AMEX: 0.738791\n",
      "[200]\ttraining's binary_logloss: 0.333025\ttraining's AMEX: 0.753374\tvalid_1's binary_logloss: 0.335438\tvalid_1's AMEX: 0.745024\n",
      "[300]\ttraining's binary_logloss: 0.292166\ttraining's AMEX: 0.758513\tvalid_1's binary_logloss: 0.295431\tvalid_1's AMEX: 0.749744\n",
      "[400]\ttraining's binary_logloss: 0.268368\ttraining's AMEX: 0.764071\tvalid_1's binary_logloss: 0.272422\tvalid_1's AMEX: 0.754394\n",
      "[500]\ttraining's binary_logloss: 0.253839\ttraining's AMEX: 0.767912\tvalid_1's binary_logloss: 0.258564\tvalid_1's AMEX: 0.758368\n",
      "[600]\ttraining's binary_logloss: 0.244309\ttraining's AMEX: 0.771712\tvalid_1's binary_logloss: 0.249679\tvalid_1's AMEX: 0.761241\n",
      "[700]\ttraining's binary_logloss: 0.23777\ttraining's AMEX: 0.775796\tvalid_1's binary_logloss: 0.24374\tvalid_1's AMEX: 0.763848\n",
      "[800]\ttraining's binary_logloss: 0.233114\ttraining's AMEX: 0.778884\tvalid_1's binary_logloss: 0.239681\tvalid_1's AMEX: 0.7669\n",
      "[900]\ttraining's binary_logloss: 0.229584\ttraining's AMEX: 0.78139\tvalid_1's binary_logloss: 0.236715\tvalid_1's AMEX: 0.76925\n",
      "[1000]\ttraining's binary_logloss: 0.226796\ttraining's AMEX: 0.783875\tvalid_1's binary_logloss: 0.234502\tvalid_1's AMEX: 0.771198\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[1000]\ttraining's binary_logloss: 0.226796\ttraining's AMEX: 0.783875\tvalid_1's binary_logloss: 0.234502\tvalid_1's AMEX: 0.771198\n",
      "Fold: 0 - seed: 0 - score 77.12%\n",
      "Fold: 0 - seed: 1\n",
      "[LightGBM] [Info] Number of positive: 59414, number of negative: 170042\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.055236 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 40080\n",
      "[LightGBM] [Info] Number of data points in the train set: 229456, number of used features: 210\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.258934 -> initscore=-1.051516\n",
      "[LightGBM] [Info] Start training from score -1.051516\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "[100]\ttraining's binary_logloss: 0.408544\ttraining's AMEX: 0.747164\tvalid_1's binary_logloss: 0.409923\tvalid_1's AMEX: 0.739546\n",
      "[200]\ttraining's binary_logloss: 0.333634\ttraining's AMEX: 0.754219\tvalid_1's binary_logloss: 0.336092\tvalid_1's AMEX: 0.744971\n",
      "[300]\ttraining's binary_logloss: 0.292576\ttraining's AMEX: 0.759618\tvalid_1's binary_logloss: 0.295899\tvalid_1's AMEX: 0.750135\n",
      "[400]\ttraining's binary_logloss: 0.268614\ttraining's AMEX: 0.764026\tvalid_1's binary_logloss: 0.272702\tvalid_1's AMEX: 0.754288\n",
      "[500]\ttraining's binary_logloss: 0.253918\ttraining's AMEX: 0.767925\tvalid_1's binary_logloss: 0.258714\tvalid_1's AMEX: 0.757747\n",
      "[600]\ttraining's binary_logloss: 0.24438\ttraining's AMEX: 0.771652\tvalid_1's binary_logloss: 0.249826\tvalid_1's AMEX: 0.76135\n",
      "[700]\ttraining's binary_logloss: 0.237849\ttraining's AMEX: 0.775653\tvalid_1's binary_logloss: 0.243925\tvalid_1's AMEX: 0.764186\n",
      "[800]\ttraining's binary_logloss: 0.233171\ttraining's AMEX: 0.778442\tvalid_1's binary_logloss: 0.239847\tvalid_1's AMEX: 0.766411\n",
      "[900]\ttraining's binary_logloss: 0.229577\ttraining's AMEX: 0.780929\tvalid_1's binary_logloss: 0.236819\tvalid_1's AMEX: 0.76926\n",
      "[1000]\ttraining's binary_logloss: 0.226772\ttraining's AMEX: 0.78353\tvalid_1's binary_logloss: 0.234569\tvalid_1's AMEX: 0.770856\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[1000]\ttraining's binary_logloss: 0.226772\ttraining's AMEX: 0.78353\tvalid_1's binary_logloss: 0.234569\tvalid_1's AMEX: 0.770856\n",
      "Fold: 0 - seed: 1 - score 77.09%\n",
      "Fold: 1 - seed: 0\n",
      "[LightGBM] [Info] Number of positive: 59414, number of negative: 170043\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.061448 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 40083\n",
      "[LightGBM] [Info] Number of data points in the train set: 229457, number of used features: 210\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.258933 -> initscore=-1.051521\n",
      "[LightGBM] [Info] Start training from score -1.051521\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "[100]\ttraining's binary_logloss: 0.409733\ttraining's AMEX: 0.743973\tvalid_1's binary_logloss: 0.410019\tvalid_1's AMEX: 0.741857\n",
      "[200]\ttraining's binary_logloss: 0.334439\ttraining's AMEX: 0.750494\tvalid_1's binary_logloss: 0.334971\tvalid_1's AMEX: 0.747546\n",
      "[300]\ttraining's binary_logloss: 0.293809\ttraining's AMEX: 0.756626\tvalid_1's binary_logloss: 0.294629\tvalid_1's AMEX: 0.752662\n",
      "[400]\ttraining's binary_logloss: 0.270199\ttraining's AMEX: 0.761238\tvalid_1's binary_logloss: 0.271316\tvalid_1's AMEX: 0.756957\n",
      "[500]\ttraining's binary_logloss: 0.255765\ttraining's AMEX: 0.765874\tvalid_1's binary_logloss: 0.257208\tvalid_1's AMEX: 0.760936\n",
      "[600]\ttraining's binary_logloss: 0.246317\ttraining's AMEX: 0.769952\tvalid_1's binary_logloss: 0.248108\tvalid_1's AMEX: 0.763702\n",
      "[700]\ttraining's binary_logloss: 0.239803\ttraining's AMEX: 0.773113\tvalid_1's binary_logloss: 0.241986\tvalid_1's AMEX: 0.766526\n",
      "[800]\ttraining's binary_logloss: 0.235198\ttraining's AMEX: 0.776372\tvalid_1's binary_logloss: 0.237814\tvalid_1's AMEX: 0.769119\n",
      "[900]\ttraining's binary_logloss: 0.231729\ttraining's AMEX: 0.779447\tvalid_1's binary_logloss: 0.234793\tvalid_1's AMEX: 0.771901\n",
      "[1000]\ttraining's binary_logloss: 0.229022\ttraining's AMEX: 0.781686\tvalid_1's binary_logloss: 0.23254\tvalid_1's AMEX: 0.773418\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[1000]\ttraining's binary_logloss: 0.229022\ttraining's AMEX: 0.781686\tvalid_1's binary_logloss: 0.23254\tvalid_1's AMEX: 0.773418\n",
      "Fold: 1 - seed: 0 - score 77.34%\n",
      "Fold: 1 - seed: 1\n",
      "[LightGBM] [Info] Number of positive: 59414, number of negative: 170043\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.050992 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 40083\n",
      "[LightGBM] [Info] Number of data points in the train set: 229457, number of used features: 210\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.258933 -> initscore=-1.051521\n",
      "[LightGBM] [Info] Start training from score -1.051521\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "[100]\ttraining's binary_logloss: 0.409575\ttraining's AMEX: 0.744784\tvalid_1's binary_logloss: 0.409859\tvalid_1's AMEX: 0.742957\n",
      "[200]\ttraining's binary_logloss: 0.335107\ttraining's AMEX: 0.751587\tvalid_1's binary_logloss: 0.335614\tvalid_1's AMEX: 0.748872\n",
      "[300]\ttraining's binary_logloss: 0.29424\ttraining's AMEX: 0.75704\tvalid_1's binary_logloss: 0.295026\tvalid_1's AMEX: 0.753853\n",
      "[400]\ttraining's binary_logloss: 0.270495\ttraining's AMEX: 0.761638\tvalid_1's binary_logloss: 0.271565\tvalid_1's AMEX: 0.757474\n",
      "[500]\ttraining's binary_logloss: 0.255831\ttraining's AMEX: 0.766146\tvalid_1's binary_logloss: 0.257223\tvalid_1's AMEX: 0.761329\n",
      "[600]\ttraining's binary_logloss: 0.246382\ttraining's AMEX: 0.769772\tvalid_1's binary_logloss: 0.248136\tvalid_1's AMEX: 0.764197\n",
      "[700]\ttraining's binary_logloss: 0.239904\ttraining's AMEX: 0.773204\tvalid_1's binary_logloss: 0.242043\tvalid_1's AMEX: 0.766618\n",
      "[800]\ttraining's binary_logloss: 0.235294\ttraining's AMEX: 0.776236\tvalid_1's binary_logloss: 0.237855\tvalid_1's AMEX: 0.769221\n",
      "[900]\ttraining's binary_logloss: 0.231787\ttraining's AMEX: 0.779489\tvalid_1's binary_logloss: 0.23479\tvalid_1's AMEX: 0.771537\n",
      "[1000]\ttraining's binary_logloss: 0.229059\ttraining's AMEX: 0.782306\tvalid_1's binary_logloss: 0.232499\tvalid_1's AMEX: 0.773547\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[1000]\ttraining's binary_logloss: 0.229059\ttraining's AMEX: 0.782306\tvalid_1's binary_logloss: 0.232499\tvalid_1's AMEX: 0.773547\n",
      "Fold: 1 - seed: 1 - score 77.35%\n"
     ]
    }
   ],
   "source": [
    "models, df_scores, importances, df_results = base_model_lgbm(train, train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>seed</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>fold_mean</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fold</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.771198</td>\n",
       "      <td>0.770856</td>\n",
       "      <td>0.771027</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.773418</td>\n",
       "      <td>0.773547</td>\n",
       "      <td>0.773483</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>seed_mean</th>\n",
       "      <td>0.772308</td>\n",
       "      <td>0.772201</td>\n",
       "      <td>0.772255</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "seed              0         1  fold_mean\n",
       "fold                                    \n",
       "0          0.771198  0.770856   0.771027\n",
       "1          0.773418  0.773547   0.773483\n",
       "seed_mean  0.772308  0.772201   0.772255"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_processed = pd.DataFrame(preprocessor.transform(test_data), index=test_data.index)\n",
    "test = test_processed.groupby('customer_ID').tail(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_list = []\n",
    "for keys in models.keys():\n",
    "    prediction_list.append(models[keys].predict(test))\n",
    "\n",
    "prediction_df = pd.DataFrame(prediction_list).T\n",
    "prediction_df.index = test.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_df.mean(axis = 1).to_csv('Data/prediction.csv')"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "dfe2e9041c24d5f91854bb9ffc35eacf9a6123f8e0fdaf39110ac9ad93cae24d"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
